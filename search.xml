<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>初见scrapy</title>
      <link href="/posts/start_scrapy.html"/>
      <url>/posts/start_scrapy.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">爬虫scrapy使用</div><h3 id="初见scrapy"><a href="#初见scrapy" class="headerlink" title="初见scrapy"></a>初见scrapy</h3><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><h3 id="新建项目"><a href="#新建项目" class="headerlink" title="新建项目"></a>新建项目</h3><pre class=" language-lang-bash"><code class="language-lang-bash">scrapy startproject scrapy_baidu_091</code></pre><h3 id="创建爬虫文件"><a href="#创建爬虫文件" class="headerlink" title="创建爬虫文件"></a>创建爬虫文件</h3><pre class=" language-lang-bash"><code class="language-lang-bash">cd startproject scrapy_baidu_091/startproject scrapy_baidu_091/spidersscrapy genspider 爬虫名字 爬取网页(不用http)eg. scrapy genspider baidu www.baidu.com</code></pre><p>成功创建了baidu文件</p><pre class=" language-lang-python"><code class="language-lang-python">import scrapyclass BaiduSpider(scrapy.Spider):    # 爬虫的名字 用于爬虫的时候使用的值    name = 'baidu'    # 允许访问的域名    allowed_domains = ['www.baidu.com']    # 起始url地址    start_urls = ['http://www.baidu.com/']    # 执行了起始start_urls之后执行的方法 response是返回对象    # 相当于response= urllib.request.urlopen    def parse(self, response):        print('不管下雨又刮风')        pass</code></pre><h3 id="运行爬虫代码"><a href="#运行爬虫代码" class="headerlink" title="运行爬虫代码"></a>运行爬虫代码</h3><pre class=" language-lang-bash"><code class="language-lang-bash">scrapy crawl 爬虫名字eg. scrapy crawl baidu</code></pre><p>取消robot协议</p><pre><code>settings.py中ROBOTSTXT_OBEY = False ，或者注释掉</code></pre><h2 id="58同城项目"><a href="#58同城项目" class="headerlink" title="58同城项目"></a>58同城项目</h2><h3 id="寻找url"><a href="#寻找url" class="headerlink" title="寻找url"></a>寻找url</h3><h4 id="打开58同城，F12打开控制台，打开网络，刷新，找到相应的网页"><a href="#打开58同城，F12打开控制台，打开网络，刷新，找到相应的网页" class="headerlink" title="打开58同城，F12打开控制台，打开网络，刷新，找到相应的网页"></a>打开58同城，F12打开控制台，打开网络，刷新，找到相应的网页</h4><p><img src="/start_scrapy.htmlUsers\25832\AppData\Roaming\Typora\typora-user-images\image-20220224113631878.png" alt="image-20220224113631878"></p><h3 id="scrapy结构"><a href="#scrapy结构" class="headerlink" title="scrapy结构"></a>scrapy结构</h3><p>项目名字</p><p>​    项目名字</p><p>​        spiders文件夹(存储爬虫文件)</p><p>​            init</p><p>​            核心功能文件</p><p>​        init</p><p>​        items 定义数据结构</p><p>​        middleware 中间件 代理</p><p>​        piplines  管道 处理下载数据</p><p>​        settings 配置文件</p><h3 id="response属性与方法"><a href="#response属性与方法" class="headerlink" title="response属性与方法"></a>response属性与方法</h3><p>response.text    字符串</p><p>response.body    二进制</p><p>response.xpath    可以直接用xpath语法</p><p>response.extract    返回selector对象的data的属性值</p><p>response.extract_first()    返回selector列表的第一个对象</p><h2 id="汽车之家项目"><a href="#汽车之家项目" class="headerlink" title="汽车之家项目"></a>汽车之家项目</h2><p>后缀是html，start_urls不能加 /</p><h3 id="scrapy工作原理"><a href="#scrapy工作原理" class="headerlink" title="scrapy工作原理"></a>scrapy工作原理</h3><ol><li>引擎向spiders要url</li><li>引擎把url给调度器</li><li>调度器把url放到队列中</li><li>从队列中取出一个请求</li><li>引擎将请求交给下载器</li><li>下载器获得互联网上的数据</li><li>下载器将数据返回引擎</li><li>引擎将数据给spiders</li><li>spiders通过xpath获得数据或Url</li><li>spiders将其交给引擎</li><li>引擎判断是数据则交给管道输出，如果是url则继续请求</li></ol><h2 id="scrapy-shell"><a href="#scrapy-shell" class="headerlink" title="scrapy shell"></a>scrapy shell</h2><pre><code>scrapy shell www.baidu.com</code></pre><p>自动进入ipython</p><pre><code>a = response.xpath(&#39;//input[@id=&quot;su&quot;]/@value&#39;)</code></pre><h2 id="当当网项目"><a href="#当当网项目" class="headerlink" title="当当网项目"></a>当当网项目</h2><p>所有的selector对象都可再次调用xpath</p><h3 id="懒加载"><a href="#懒加载" class="headerlink" title="懒加载"></a>懒加载</h3><p>图片刷新到才加载src，注意换一下</p><h3 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h3><p>迭代器</p><p>如果想使用管道需要在settings里打开</p><pre><code># 爬虫文件执行前def open_spider(self, spider):# 爬虫文件执行后def close_spider(self, spider):</code></pre><h3 id="多条管道同时"><a href="#多条管道同时" class="headerlink" title="多条管道同时"></a>多条管道同时</h3><ol><li>定义管道类</li><li>在settings中开启</li></ol><p>如果拿不到数据要检查xpath语法是否正确</p><h2 id="读书网项目"><a href="#读书网项目" class="headerlink" title="读书网项目"></a>读书网项目</h2><pre><code>scrapy genspider -t crawl read https://www.dushu.com/book/1617.html</code></pre><p>注意首页格式</p><h3 id="数据配置"><a href="#数据配置" class="headerlink" title="数据配置"></a>数据配置</h3><p>打开settings</p><pre class=" language-lang-python"><code class="language-lang-python">DB_HOST = '127.0.0.1'DB_PORT = 3306DB_USER = 'root'DB_PASSWORD = '12345678'DB_NAME = 'cba_k8_test'DB_CHARSET = 'utf8'  # 注意没有杠</code></pre><pre class=" language-lang-python"><code class="language-lang-python"># Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html# useful for handling different item types with a single interfacefrom itemadapter import ItemAdapterclass ScrapyReadbook101Pipeline:    def __init__(self):        self.fp = open('book.json', 'w', encoding='utf-8')    def process_item(self, item, spider):        self.fp.write(str(item))        return item    def close_spider(self, spider):        self.fp.close()# 加载seetings文件from scrapy.utils.project import get_project_settingsimport pymysqlclass MysqlPipeline:    def __init__(self):        settings = get_project_settings()        self.host = settings['DB_HOST']        self.port = settings['DB_PORT']        self.user = settings['DB_USER']        self.password = settings['DB_PASSWORD']        self.name = settings['DB_NAME']        self.charset = settings['DB_CHARSET']        self.conn = ''        self.cursor = ''        self.connect()    def connect(self):        self.conn = pymysql.connect(            host=self.host,            port=self.port,            user=self.user,            password=self.password,            db=self.name,            charset=self.charset        )        self.cursor = self.conn.cursor()    def process_item(self, item, spider):        sql = 'insert into book(name, src) values("&#123;&#125;", "&#123;&#125;")'.format(item['name'], item['src'])        # 执行与提交        self.cursor.execute(sql)        self.conn.commit()        return item    def close_spider(self, spider):        self.cursor.close()        self.conn.close()</code></pre><h3 id="日志文件"><a href="#日志文件" class="headerlink" title="日志文件"></a>日志文件</h3><pre class=" language-lang-python"><code class="language-lang-python">LOG_FILE = 'logdemo.log' # .log结尾</code></pre><h3 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h3><pre class=" language-lang-python"><code class="language-lang-python">def start_requests(self):    url = 'xxx'    data = &#123;        'kw': 'final'    &#125;    yield scrapy.FormRequest(url=url, formate=data, callback=sekf.parse_second)def parse_second(self, response):    content = response.text</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spacy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/posts/lc-note.html"/>
      <url>/posts/lc-note.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">leetcode刷题笔记</div><h1 id="leetcode"><a href="#leetcode" class="headerlink" title="leetcode"></a>leetcode</h1><p>注意python中队列deque使用</p><p>数字处理时学会想到用二进制</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/posts/python-note.html"/>
      <url>/posts/python-note.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">python的小笔记(不定时更新)</div><h1 id="python"><a href="#python" class="headerlink" title="python"></a>python</h1><h2 id="enumerate"><a href="#enumerate" class="headerlink" title="enumerate"></a>enumerate</h2><p>迭代器，顺便把序号给标上了</p><pre class=" language-lang-python"><code class="language-lang-python">for index, i in enumerate(range(100000)):    if index % 2 == 0:        s *= i</code></pre><h2 id="tqdm"><a href="#tqdm" class="headerlink" title="tqdm"></a>tqdm</h2><p>进度条</p><pre class=" language-lang-python"><code class="language-lang-python">for index, i in tqdm(enumerate(range(100000))):    time.sleep(0.1)    if index % 2 == 0:        s *= i</code></pre><p><img src="/python-note.htmlnekoblog\source\_posts\python-note\tqdm.png" alt></p><h2 id="time"><a href="#time" class="headerlink" title="time"></a>time</h2><p>时间模块</p><h3 id="time-process-time"><a href="#time-process-time" class="headerlink" title="time.process_time()"></a>time.process_time()</h3><p>用来记录程序运行的cpu时间</p><pre class=" language-lang-python"><code class="language-lang-python">t_start = time.process_time()t_end = time.process_time()print('enumerate: &#123;&#125;'.format(t_end-t_start))</code></pre><h2 id="str相关"><a href="#str相关" class="headerlink" title="str相关"></a>str相关</h2><h3 id="判断数字大小写"><a href="#判断数字大小写" class="headerlink" title="判断数字大小写"></a>判断数字大小写</h3><pre><code>字符串.isalnum()  所有字符都是数字或者字母，为真返回 Ture，否则返回 False。字符串.isalpha()   所有字符都是字母，为真返回 Ture，否则返回 False。字符串.isdigit()     所有字符都是数字，为真返回 Ture，否则返回 False。字符串.islower()    所有字符都是小写，为真返回 Ture，否则返回 False。字符串.isupper()   所有字符都是大写，为真返回 Ture，否则返回 False。字符串.istitle()      所有单词都是首字母大写，为真返回 Ture，否则返回 False。字符串.isspace()   所有字符都是空白字符，为真返回 Ture，否则返回 False。</code></pre><h3 id="三个单引号可以使用有换行符的字符串"><a href="#三个单引号可以使用有换行符的字符串" class="headerlink" title="三个单引号可以使用有换行符的字符串"></a>三个单引号可以使用有换行符的字符串</h3><pre class=" language-lang-python"><code class="language-lang-python">s = '''澎湃在线 109358潇湘晨报 88460光明网 51669新浪财经 36274手机中国网 33604北青网 24698环球网 23501中国经济网 17933东方财富网 12972中国新闻网 12900央广网 11406中国青年网 11167环京津网 10834新华网客户端 10583上观新闻 10556天眼新闻 10553红网 10480闪电新闻 10474新华社图片 10348新华社 10198湖南24小时 9823中国日报网 9409海报新闻 9028新京报 8297每日经济新闻 7694中国江苏网 7230浙江新闻 7137教育信息速报 7028界面新闻 6878证券之星 6867新民晚报 6678国际在线 6626新华社新媒体 6527河北新闻网 6112北京日报客户端 6056人民网 5836新华社客户端 5738'''print(s)</code></pre><h2 id="list相关"><a href="#list相关" class="headerlink" title="list相关"></a>list相关</h2><h3 id="sort函数"><a href="#sort函数" class="headerlink" title="sort函数"></a>sort函数</h3><pre class=" language-lang-python"><code class="language-lang-python">list.sort( key=None, reverse=False)</code></pre><p>key一般用lambda函数</p><pre class=" language-lang-python"><code class="language-lang-python">properties.sort(key=lambda i: i[0], reverse=True)</code></pre><p>此外若排序的列表是多重元素，那么会按元素顺序依次比较</p><pre class=" language-lang-python"><code class="language-lang-python">test = [[2,2],[3,3]]</code></pre><h3 id="deque队列"><a href="#deque队列" class="headerlink" title="deque队列"></a>deque队列</h3><pre class=" language-lang-python"><code class="language-lang-python">from collections import dequequeue = deque(["Eric", "John", "Michael"])queue.append("Terry")           # Terry arrivesqueue.append("Graham")          # Graham arrivesqueue.popleft()                 # The first to arrive now leavesqueue.popleft()                 # The second to arrive now leavesqueue                           # Remaining queue in order of arrival</code></pre><h2 id="num相关"><a href="#num相关" class="headerlink" title="num相关"></a>num相关</h2><h3 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h3><pre class=" language-lang-python"><code class="language-lang-python">// 整数整除% 取余</code></pre><h2 id="文件相关"><a href="#文件相关" class="headerlink" title="文件相关"></a>文件相关</h2><h3 id="文件打开"><a href="#文件打开" class="headerlink" title="文件打开"></a>文件打开</h3><pre class=" language-lang-python"><code class="language-lang-python">self.fp = open('player_match_data.json', 'w', encoding='utf-8')</code></pre><h3 id="读取当前位置"><a href="#读取当前位置" class="headerlink" title="读取当前位置"></a>读取当前位置</h3><pre class=" language-lang-python"><code class="language-lang-python">self.fp.tell()</code></pre><h3 id="改变文件指针位置"><a href="#改变文件指针位置" class="headerlink" title="改变文件指针位置"></a>改变文件指针位置</h3><pre class=" language-lang-python"><code class="language-lang-python">self.fp.seek(self.fp.tell()-1)  # 回退一格</code></pre><h3 id="删除文件该位置之后的内容"><a href="#删除文件该位置之后的内容" class="headerlink" title="删除文件该位置之后的内容"></a>删除文件该位置之后的内容</h3><pre class=" language-lang-python"><code class="language-lang-python">self.fp.truncate()</code></pre>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>git配置多个ssh key</title>
      <link href="/posts/git_ssh.html"/>
      <url>/posts/git_ssh.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">github/gitlab(内网) 管理多个ssh key解决报错问题</div><p>在github下成功配置后在gitlab(内网服务器)下一直不成功，这里写一下解决策略</p><h1 id="git配置多个ssh-key"><a href="#git配置多个ssh-key" class="headerlink" title="git配置多个ssh key"></a>git配置多个ssh key</h1><h2 id="普通安装"><a href="#普通安装" class="headerlink" title="普通安装"></a>普通安装</h2><h3 id="添加第一个ssh-key"><a href="#添加第一个ssh-key" class="headerlink" title="添加第一个ssh key"></a>添加第一个ssh key</h3><p>首先绑定用户就不提了，绑定完之后生成ssh key</p><pre class=" language-lang-bash"><code class="language-lang-bash">$ ssh-keygen -t rsa -C "youremail@yourcompany.com"</code></pre><p>生成后会在 ~/.ssh/ 目录下生成 id_rsa 和 id_rsa.pub 两个文件，id_rsa是私钥不能给别人看， id_rsa.pub是公钥</p><h3 id="绑定github-gitlab"><a href="#绑定github-gitlab" class="headerlink" title="绑定github(gitlab)"></a>绑定github(gitlab)</h3><p>打开id_rsa.pub，将其内容复制</p><p>然后打开github(gitlab)网站，进入setting(设置)，找到ssh相关设置，添加新的ssh key,将其内容添加进去</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>将github.com改成相应的网址即使</p><pre class=" language-lang-bash"><code class="language-lang-bash">$ ssh -T git@github.com</code></pre><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><p>普通的教程到上面就结束了，我绑定github也非常的正常，但绑定实验室内网的gitlab却一直失败，最后使用配置文件成功</p><p>首先打开~/.ssh/ 目录，里面会有一个config文件(没有就创建一个，没有任何后缀)，将其打开，进行编辑</p><pre><code># gitlabHost gitlab.com    HostName gitlab.com    PreferredAuthentications publickey    IdentityFile ~/.ssh/id_rsa    Port 666# githubHost github.com    HostName github.com    PreferredAuthentications publickey    IdentityFile ~/.ssh/id_rsa_github    Port 443</code></pre><p>首先通过这个文件可以配置不同的账户，将Host和HostName改成相应的内网域名即可，IdentityFile 用来在不同网站使用不同的ssh key，最后是端口，弄了半天才知道内网的服务器端口不是经常用的443或者22，而是别的端口，大家在使用的时候一定要问清楚</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初见spacy</title>
      <link href="/posts/start_spacy.html"/>
      <url>/posts/start_spacy.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">nlp工具spacy库的使用</div><h3 id="初见spacy"><a href="#初见spacy" class="headerlink" title="初见spacy"></a>初见spacy</h3><p>由于使用stanfordcorenlp时一直报错，不得已改用spacy</p><h4 id="关于spacy及其安装"><a href="#关于spacy及其安装" class="headerlink" title="关于spacy及其安装"></a>关于spacy及其安装</h4><p>Spacy 是由 <code>cython</code> 编写。因此它是一个非常快的库。 <code>spaCy</code> 提供简洁的接口用来访问其方法和属性 governed by trained machine (and deep) learning models.</p><p><a href="https://spacy.io/" class="LinkCard" target="_blank">spacy官网</a></p><h5 id="spacy安装"><a href="#spacy安装" class="headerlink" title="spacy安装"></a>spacy安装</h5><p>pycharm下直接安装即可，其他方式也可用命令行</p><pre><code> pip install spacy</code></pre><p>第一次用python的小伙伴pip速度慢建议搜一下更换pip国内源 <a href="https://blog.csdn.net/doulihang/article/details/87193010" class="LinkCard" target="_blank">pip更换源教程</a></p><h5 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h5><p><a href="https://spacy.io/usage/models" class="LinkCard" target="_blank">spacy官网模型教程</a></p><p>由于大部分小伙伴主要还是做中文处理，所以还要安装中文的模型</p><pre><code>python -m spacy download zh_core_web_sm</code></pre><p>但直接按上述方式下载由于不可抗力会报错，需要科学上网，如果打开了科学上网还不行的同学应该是没有配置命令行的代理，按下面的步骤在命令行下执行（10809是科学上网的端口号,win10搜索代理进入相关设置观看）</p><pre><code>set HTTP_PROXY=http://127.0.0.1:10809set HTTPS_PROXY=http://127.0.0.1:10809</code></pre><h4 id="spacy初使用"><a href="#spacy初使用" class="headerlink" title="spacy初使用"></a>spacy初使用</h4><pre><code>import spacynlp = spacy.load(&quot;zh_core_web_sm&quot;)doc = nlp(u&#39;不管下雨又刮风，泥巴路上花一朵&#39;)for token in doc:    print(token)for token in doc:    print(token, token.pos_, token.pos)</code></pre><p>结果<br><img src="/posts/start_spacy/spacy_test.png" class title="spacy_test"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spacy </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是篮球模板</title>
      <link href="/posts/basketball.html"/>
      <url>/posts/basketball.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，篮球篮球篮球。</div><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ul><li>项目1</li><li>项目2</li><li>项目3</li></ul><p>正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文</p><p><img src="/l/图片url" alt="图片介绍"></p><p><code>正文一些需要高亮色的文字</code></p><p><a href="链接地址" class="LinkCard" target="_blank">引入链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 篮球 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 篮球 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NBA交易评级回顾</title>
      <link href="/posts/nba-trade-review.html"/>
      <url>/posts/nba-trade-review.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">回顾NBA往年的交易评级，看看自己与专家们的看法的区别在哪，以及专家们未曾想象的事情</div><h1 id="NBA交易评级回顾（事后诸葛亮）"><a href="#NBA交易评级回顾（事后诸葛亮）" class="headerlink" title="NBA交易评级回顾（事后诸葛亮）"></a>NBA交易评级回顾（事后诸葛亮）</h1><h2 id="2018年"><a href="#2018年" class="headerlink" title="2018年"></a>2018年</h2><h3 id="格里芬大交易"><a href="#格里芬大交易" class="headerlink" title="格里芬大交易"></a>格里芬大交易</h3><p><img src="/nba-trade-review.htmlUsers\25832\AppData\Roaming\Typora\typora-user-images\image-20220211220153118.png" alt="image-20220211220153118"></p><h4 id="法尔克"><a href="#法尔克" class="headerlink" title="法尔克"></a>法尔克</h4><p><a href="https://weibo.com/ttarticle/p/show?id=2309404202289719061474">法尔克评格里芬大交易：是以退为进，还是没有选择的选择？ (weibo.com)</a></p><p>法尔克对这笔交易看法比较平淡，认为促成这个交易的是之前多年的积累，而这笔交易也改变不了两只球队的现状</p><h4 id="洛维"><a href="#洛维" class="headerlink" title="洛维"></a>洛维</h4><p><a href="https://weibo.com/ttarticle/p/show?id=2309404201914249179563">洛维评格里芬大交易：一场超级平庸的豪赌 (weibo.com)</a></p><p>洛维认为快船进行的大刀阔斧的重建，而活塞则进行了一场豪赌，而如果活塞可以利用现有的班底进几年季后赛，还是可以接受的</p><h4 id="佩尔顿"><a href="#佩尔顿" class="headerlink" title="佩尔顿"></a>佩尔顿</h4><p><a href="https://weibo.com/ttarticle/p/show?id=2309404201820644854942">格里芬交易评级：活塞D+，快船B+ (weibo.com)</a></p><pre><code>快船B+ 活塞D</code></pre><p>活塞：格里芬可能不值得用哈里斯换，而且可能在30岁后下滑，薪资结构也变差了</p><h4 id="个人回顾"><a href="#个人回顾" class="headerlink" title="个人回顾"></a>个人回顾</h4><h5 id="快船"><a href="#快船" class="headerlink" title="快船"></a>快船</h5><p>哈里斯：快船利用他打了一年多球后转手将他交易给76人得到了沙梅特，穆斯卡拉(换来了祖巴茨)，两个首轮（选了曼恩以及后面用来交易乔治的财产）</p><p>2018年首轮签：选了小乔，然后立刻交易成了亚历山大，后者是乔治交易最主要财产</p><p>快船通过这笔交易，让人们知道什么叫单车变摩托，这笔交易后的唯一代价似乎只有那一年没有进入季后赛，这笔交易最后换来的所有资产基本都变成了19年乔治那个大交易的筹码，也就是说快船用<strong>格里芬+一年季后赛没进+一两个后顺位的热火首轮签=乔治+祖巴茨</strong>，在一两年内完成了大重建</p><h5 id="活塞"><a href="#活塞" class="headerlink" title="活塞"></a>活塞</h5><p>活塞在这笔资产中获得的唯一资产就是格里芬，而正如佩尔顿所料，哈里斯在第二年就成长为跟格里芬差不太多的球员(有格里芬下滑也有哈里斯成长)，而活塞也非常不幸的只通过格里芬进了一届季后赛，且首轮就出局，最后与格里芬进行买断，目前依旧在黑暗四天王中</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p><strong>择其而上得其中 择其而中得其下</strong>，活塞非常好的阐释了这一点，球队因为多年没有进季后赛，想通过格里芬进几年季后赛即可，但天不遂人愿，格里芬光速下滑，活塞也只通过他进了一年季后赛，所谓饮鸩止渴。如果不做这笔交易，不提选秀选到小乔和亚历山大这种，至少哈里斯在第二年就能打出跟格里芬差不多的贡献（出勤加单位时间），更别提薪资空间和选秀权这种一大票东西了。</p>]]></content>
      
      
      <categories>
          
          <category> 篮球 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 篮球 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是技术文章模板</title>
      <link href="/posts/articletemplate.html"/>
      <url>/posts/articletemplate.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><div class="note info">前言，技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言技术文章前言。</div><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><ul><li>项目1</li><li>项目2</li><li>项目3</li></ul><p>正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文正文</p><p><img src="/l/图片url" alt="图片介绍"></p><p><code>正文一些需要高亮色的文字</code></p><p><a href="链接地址" class="LinkCard" target="_blank">引入链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>这是随笔文章模板</title>
      <link href="/posts/essay-demo.html"/>
      <url>/posts/essay-demo.html</url>
      
        <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><h4 id="这是博主的随笔板块，后续会更新文章，目前弄好了模板格式"><a href="#这是博主的随笔板块，后续会更新文章，目前弄好了模板格式" class="headerlink" title="这是博主的随笔板块，后续会更新文章，目前弄好了模板格式~"></a>这是博主的随笔板块，后续会更新文章，目前弄好了模板格式~</h4><div class="ipage">    <div class="ititle">节选自《你若安好便是晴天》- 前言</div>  <div class="izhengwen">  <p>几场梅雨，几卷荷风，江南已是烟水迷离。小院里湿润的青苔在雨中纯净生长。这个季节，许多人都在打听关于莲荷的消息，以及茉莉在黄昏浮动的神秘幽香。不知多少人会记得有个女子，曾经走过人间四月天，又与莲开的夏季有过相濡以沫的约定。</p>  <p>一个人，一本书，一杯茶，一帘梦。有时候，寂寞是这样叫人心动，也只有此刻，世事才会如此波澜不惊。凉风吹起书页，这烟雨让尘封在书卷里的词章和故事弥漫着潮湿的气息。独倚幽窗，看转角处的青石小巷，一柄久违的油纸伞，遮住了低过屋檐的光阴。</p>  <p>时光微凉，那一场远去的往事被春水浸泡，秋风吹拂，早已洗去铅华，清绝明净。以为历经人生匆匆聚散，尝过尘世种种烟火，应该承担岁月带给我们的沧桑。可流年分明安然无恙，而山石草木是这样毫发无伤。只是曾经许过地老天荒的城，在细雨中越发地清瘦单薄。</p>  <p>青梅煎好的茶水，还是当年的味道；而我们等候的人，不会再来。后来才知道，那一袭素色白衣的女子已化身为燕，去寻觅水乡旧巢。她走过的地方，有一树一树的花开，她呢喃的梁间，还留着余温犹存的梦。有人说，她是个冰洁的女子，所以无论人世如何变迁，她都有着美丽的容颜。有人说，她是个理智的女子，不管面临怎样的诱惑，最后都可以全身而退。</p>  <p>她叫林徽因，出生于杭州，是许多人梦中期待的白莲。她在雨雾之都伦敦，发生过一场空前绝后的康桥之恋。她爱过三个男子，爱得清醒，也爱得平静。徐志摩为她徜徉在康桥，深情地等待一场旧梦可以归来。梁思成与她携手走过千山万水，为完成使命而相约白头。金岳霖为她终身不娶，痴心不改地守候一世。可她懂得人生飘忽不定，要学会随遇而安。</p>  </div></div>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
